{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBlkTreEQS04"
      },
      "source": [
        "Pour ce nouveau TP, nous allons poursuivre  l'utilisation des Transformers pour le traitement du langage naturel à travers la tâche devenue emblématique du domaine : la **génération de texte**.\n",
        "\n",
        "Pour cela nous allons utiliser une base de données de tweets écrits par Donald Trump sur son compte twitter (@realdonaldtrump) entre 2009 et 2020.\n",
        "\n",
        "<center><img src=\"https://www.alternatives-economiques.fr/sites/default/files/public/styles/16x9/public/field/image/000_36tp6xn.webp?orig=jpg&itok=bKKkrbct\" width=600> </center>\n",
        "\n",
        "A l'issue de l'entraînement, on obtiendra un modèle permettant de générer, par exemple, les phrases suivantes :\n",
        "\n",
        "```I love Putin. He is a great guy. But I think he is a weak man. I see him as a very tough guy. He is not very smart, but he is very strong. He has a lot of things to say.``` (obtenu à partir d'une phrase de départ 'I love Putin')\n",
        "\n",
        "```Obama is  the most corrupt, the most corrupt President ever. As a private citizen I have no interest in paying him any money. He is a fraud!``` (obtenu à partir de la phrase de départ 'Obama is')\n",
        "\n",
        "**NB:** Vous allez voir que la quantité de code à compléter dans ce TP est relativement faible. Si vous terminez en avance, n'hésitez pas à réfléchir à comment adapter ce code à votre projet !\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7k2_Q2Bcevc"
      },
      "source": [
        "# Présentation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tmi5HK0QjOs"
      },
      "source": [
        "Commencez par télécharger les données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "36H07MywchIW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-01 08:36:00.226228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743489360.239701    9384 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743489360.243481    9384 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-01 08:36:00.259048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!', 'Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!', 'Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman: http://tinyurl.com/ooafwn - Very funny!', 'New Blog Post: Celebrity Apprentice Finale and Lessons Learned Along the Way: http://tinyurl.com/qlux5e', '\"My persona will never be that of a wallflower - I’d rather build walls than cling to them\" --Donald J. Trump', 'Miss USA Tara Conner will not be fired - \"I\\'ve always been a believer in second chances.\" says Donald Trump', 'Listen to an interview with Donald Trump discussing his new book, Think Like A Champion: http://tinyurl.com/qs24vl', '\"Strive for wholeness and keep your sense of wonder intact.\" --Donald J. Trump http://tinyurl.com/pqpfvm', 'Enter the \"Think Like A Champion\" signed book and keychain contest: http://www.trumpthinklikeachampion.com/contest/', '\"When the achiever achieves, it\\'s not a plateau, it’s a beginning.\" --Donald J. Trump http://tinyurl.com/pqpfvm']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, Model\n",
        "from keras import models\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Téléchargement des données\n",
        "path_to_file = tf.keras.utils.get_file('realdonaltrump.csv', 'https://drive.google.com/uc?export=download&id=1s1isv9TQjGiEr2gG__8bOdBFvQlmepRt')\n",
        "\n",
        "# Lecture du CSV dans une liste de tweets et concaténation des tweets dans la variable text\n",
        "tweets = []\n",
        "text = ''\n",
        "with open(path_to_file, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        tweets.append(row['content'])\n",
        "        text += row['content']\n",
        "\n",
        "\n",
        "# Affichage des 10 premiers tweets\n",
        "print(tweets[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVMw_6NEQmfq"
      },
      "source": [
        "Le fichier CSV compte 43339 tweets, accompagnés d'autres métadonnées que nous n'utiliserons pas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "foL5SND0sMQG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longueur totale du texte: 5699025 caractères\n"
          ]
        }
      ],
      "source": [
        "# Nombre total de caractères du dataset\n",
        "print(f'Longueur totale du texte: {len(text)} caractères')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LVgg7F6gsg-w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147 unique caractères\n"
          ]
        }
      ],
      "source": [
        "# Extraction des caractères uniques du texte\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique caractères')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IKo3YevosjVC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " : 856303\n",
            "e: 455638\n",
            "t: 351240\n",
            "a: 332952\n",
            "o: 319373\n",
            "n: 282372\n",
            "r: 272517\n",
            "i: 272434\n",
            "s: 228192\n",
            "l: 185417\n",
            "h: 168652\n",
            "d: 139024\n",
            "u: 129717\n",
            "m: 118139\n",
            "c: 109390\n",
            "p: 100221\n",
            "y: 88739\n",
            "g: 86469\n",
            "w: 84829\n",
            ".: 81920\n",
            "f: 64670\n",
            "b: 63188\n",
            "T: 50464\n",
            "@: 42086\n",
            "v: 41635\n",
            "k: 41393\n",
            "/: 41364\n",
            ",: 30662\n",
            "A: 30559\n",
            "!: 27575\n",
            "I: 27032\n",
            "S: 26053\n",
            "D: 25546\n",
            "\": 25353\n",
            "C: 24821\n",
            "-: 23461\n",
            ":: 22506\n",
            "M: 20969\n",
            "N: 20163\n",
            "1: 19603\n",
            "R: 18201\n",
            "0: 17780\n",
            "E: 17346\n",
            "O: 16117\n",
            "P: 15578\n",
            "W: 15463\n",
            "B: 15334\n",
            "2: 14770\n",
            "G: 14510\n",
            "H: 13474\n",
            "L: 11757\n",
            "F: 11189\n",
            "x: 9812\n",
            "6: 9699\n",
            "': 9550\n",
            "5: 9233\n",
            "U: 9143\n",
            "j: 9124\n",
            "J: 8708\n",
            "3: 8276\n",
            "4: 8208\n",
            "7: 7927\n",
            "#: 7794\n",
            "9: 7367\n",
            "8: 7338\n",
            "Y: 6853\n",
            "V: 6500\n",
            "z: 6251\n",
            "K: 5723\n",
            "’: 5311\n",
            "&: 4792\n",
            "…: 4293\n",
            "?: 4196\n",
            "_: 4059\n",
            "”: 3259\n",
            "“: 3257\n",
            "q: 2872\n",
            "): 2631\n",
            "(: 2563\n",
            "Z: 1813\n",
            "X: 1803\n",
            "Q: 1616\n",
            "%: 1098\n",
            "$: 1019\n",
            "=: 694\n",
            "—: 611\n",
            "–: 610\n",
            "‘: 258\n",
            ";: 117\n",
            "~: 116\n",
            "+: 107\n",
            "*: 37\n",
            "|: 34\n",
            ">: 19\n",
            "<: 16\n",
            "•: 16\n",
            "é: 13\n",
            "♡: 13\n",
            "[: 7\n",
            "]: 7\n",
            "★: 6\n",
            "£: 5\n",
            "―: 5\n",
            "®: 5\n",
            "»: 5\n",
            "º: 4\n",
            "«: 4\n",
            "ı: 4\n",
            "´: 4\n",
            "}: 4\n",
            "☞: 3\n",
            "{: 3\n",
            "ō: 3\n",
            "ó: 3\n",
            "‏: 2\n",
            "‎: 2\n",
            "è: 2\n",
            "É: 2\n",
            "í: 2\n",
            "ï: 2\n",
            "`: 2\n",
            "½: 2\n",
            "ö: 2\n",
            "ô: 2\n",
            "ー: 2\n",
            "€: 1\n",
            "\\: 1\n",
            "Ｒ: 1\n",
            "Ｔ: 1\n",
            ": 1\n",
            "™: 1\n",
            "′: 1\n",
            "●: 1\n",
            "《: 1\n",
            "☆: 1\n",
            "ĺ: 1\n",
            "ñ: 1\n",
            "􏰀: 1\n",
            "☉: 1\n",
            "á: 1\n",
            "ø: 1\n",
            "ğ: 1\n",
            "ễ: 1\n",
            "â: 1\n",
            "ú: 1\n",
            "➜: 1\n",
            "ò: 1\n"
          ]
        }
      ],
      "source": [
        "# Calcul des fréquences d'apparition de chaque caractère\n",
        "char_freq = {}\n",
        "for char in text:\n",
        "  if char not in char_freq:\n",
        "    char_freq[char] = 1\n",
        "  else:\n",
        "    char_freq[char] += 1\n",
        "\n",
        "\n",
        "# Affichage du vocabulaire trié par ordre de fréquence décroissant en filtrant les caractères apparaissant moins de 100 fois\n",
        "for char in sorted(char_freq, key=lambda x: char_freq[x], reverse=True):\n",
        "  print(f'{char}: {char_freq[char]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkPqVY9p9HhR"
      },
      "source": [
        "Nous avons donc 147 caractères dans la base de données, dont certains sont extrêmement rares. Il aurait certainement été possible de remplacer les caractères rares par un token particulier (\\<unk\\>, pour *unknown*) mais ils sont suffisamment rares pour ne pas causer de problèmes particuliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUfnsU85t10M"
      },
      "source": [
        "# Entraînement complet d'un modèle relativement petit\n",
        "\n",
        "## Tokenization et formatage des données\n",
        "\n",
        "Pour plus de simplicité dans cette partie, nous allons adopter une *Tokenization* basée sur les caractères. Chaque caractère se verra assigner un identifiant unique, et nous allons ajouter deux tokens supplémentaires, l'un pour désigner le début d'un tweet (\\<sot\\>, *start of tweet*), l'autre pour en désigner la fin (\\<eot\\>).\n",
        "\n",
        "Nous allons créer deux structures de données permettant de passer d'une représentation caractère vers les indices des tokens (```char_to_ind```) et vice versa (```ind_to_char```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g31ZFDnV9BMA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '<': 30, '=': 31, '>': 32, '?': 33, '@': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, '\\\\': 62, ']': 63, '_': 64, '`': 65, 'a': 66, 'b': 67, 'c': 68, 'd': 69, 'e': 70, 'f': 71, 'g': 72, 'h': 73, 'i': 74, 'j': 75, 'k': 76, 'l': 77, 'm': 78, 'n': 79, 'o': 80, 'p': 81, 'q': 82, 'r': 83, 's': 84, 't': 85, 'u': 86, 'v': 87, 'w': 88, 'x': 89, 'y': 90, 'z': 91, '{': 92, '|': 93, '}': 94, '~': 95, '\\x92': 96, '£': 97, '«': 98, '®': 99, '´': 100, 'º': 101, '»': 102, '½': 103, 'É': 104, 'á': 105, 'â': 106, 'è': 107, 'é': 108, 'í': 109, 'ï': 110, 'ñ': 111, 'ò': 112, 'ó': 113, 'ô': 114, 'ö': 115, 'ø': 116, 'ú': 117, 'ğ': 118, 'ı': 119, 'ĺ': 120, 'ō': 121, 'ễ': 122, '\\u200e': 123, '\\u200f': 124, '–': 125, '—': 126, '―': 127, '‘': 128, '’': 129, '“': 130, '”': 131, '•': 132, '…': 133, '′': 134, '€': 135, '™': 136, '●': 137, '★': 138, '☆': 139, '☉': 140, '☞': 141, '♡': 142, '➜': 143, '《': 144, 'ー': 145, 'Ｒ': 146, 'Ｔ': 147, '\\U0010fc00': 148, '<sot>': 0, '<eot>': 1}\n",
            "{2: ' ', 3: '!', 4: '\"', 5: '#', 6: '$', 7: '%', 8: '&', 9: \"'\", 10: '(', 11: ')', 12: '*', 13: '+', 14: ',', 15: '-', 16: '.', 17: '/', 18: '0', 19: '1', 20: '2', 21: '3', 22: '4', 23: '5', 24: '6', 25: '7', 26: '8', 27: '9', 28: ':', 29: ';', 30: '<', 31: '=', 32: '>', 33: '?', 34: '@', 35: 'A', 36: 'B', 37: 'C', 38: 'D', 39: 'E', 40: 'F', 41: 'G', 42: 'H', 43: 'I', 44: 'J', 45: 'K', 46: 'L', 47: 'M', 48: 'N', 49: 'O', 50: 'P', 51: 'Q', 52: 'R', 53: 'S', 54: 'T', 55: 'U', 56: 'V', 57: 'W', 58: 'X', 59: 'Y', 60: 'Z', 61: '[', 62: '\\\\', 63: ']', 64: '_', 65: '`', 66: 'a', 67: 'b', 68: 'c', 69: 'd', 70: 'e', 71: 'f', 72: 'g', 73: 'h', 74: 'i', 75: 'j', 76: 'k', 77: 'l', 78: 'm', 79: 'n', 80: 'o', 81: 'p', 82: 'q', 83: 'r', 84: 's', 85: 't', 86: 'u', 87: 'v', 88: 'w', 89: 'x', 90: 'y', 91: 'z', 92: '{', 93: '|', 94: '}', 95: '~', 96: '\\x92', 97: '£', 98: '«', 99: '®', 100: '´', 101: 'º', 102: '»', 103: '½', 104: 'É', 105: 'á', 106: 'â', 107: 'è', 108: 'é', 109: 'í', 110: 'ï', 111: 'ñ', 112: 'ò', 113: 'ó', 114: 'ô', 115: 'ö', 116: 'ø', 117: 'ú', 118: 'ğ', 119: 'ı', 120: 'ĺ', 121: 'ō', 122: 'ễ', 123: '\\u200e', 124: '\\u200f', 125: '–', 126: '—', 127: '―', 128: '‘', 129: '’', 130: '“', 131: '”', 132: '•', 133: '…', 134: '′', 135: '€', 136: '™', 137: '●', 138: '★', 139: '☆', 140: '☉', 141: '☞', 142: '♡', 143: '➜', 144: '《', 145: 'ー', 146: 'Ｒ', 147: 'Ｔ', 148: '\\U0010fc00', 0: '<sot>', 1: '<eot>'}\n"
          ]
        }
      ],
      "source": [
        "# Création du dictionnaire chatr_to_ind pour associer un indice unique à chaque caractère, en commençant à l'indice 2\n",
        "char_to_ind = {char: idx+2 for idx, char in enumerate(vocab)}\n",
        "char_to_ind['<sot>'] = 0 # Start of tweet\n",
        "char_to_ind['<eot>'] = 1 # End of tweet\n",
        "\n",
        "# Création du dictionnaire ind_to_char pour associer à chaque indice un caractère\n",
        "ind_to_char = {idx: char for char, idx in char_to_ind.items()}\n",
        "\n",
        "\n",
        "print(char_to_ind)\n",
        "print(ind_to_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iio2rA4I9Agp"
      },
      "source": [
        "Nous pouvons maintenant passer à la Tokenization de la base de données, c'est-à-dire à la conversion de chaque tweet en une liste d'identifiants représentant le tweet. Par exemple, pour le premier tweet :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "avKG5oZg-ni0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n",
            "[0, 36, 70, 2, 84, 86, 83, 70, 2, 85, 80, 2, 85, 86, 79, 70, 2, 74, 79, 2, 66, 79, 69, 2, 88, 66, 85, 68, 73, 2, 38, 80, 79, 66, 77, 69, 2, 54, 83, 86, 78, 81, 2, 80, 79, 2, 46, 66, 85, 70, 2, 48, 74, 72, 73, 85, 2, 88, 74, 85, 73, 2, 38, 66, 87, 74, 69, 2, 46, 70, 85, 85, 70, 83, 78, 66, 79, 2, 66, 84, 2, 73, 70, 2, 81, 83, 70, 84, 70, 79, 85, 84, 2, 85, 73, 70, 2, 54, 80, 81, 2, 54, 70, 79, 2, 46, 74, 84, 85, 2, 85, 80, 79, 74, 72, 73, 85, 3, 1]\n"
          ]
        }
      ],
      "source": [
        "print(tweets[0])\n",
        "\n",
        "tokenized_tweet_0 = [char_to_ind['<sot>']]\n",
        "for char in tweets[0]:\n",
        "  tokenized_tweet_0.append(char_to_ind[char])\n",
        "tokenized_tweet_0.append(char_to_ind['<eot>'])\n",
        "\n",
        "print(tokenized_tweet_0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peuGYPjw-0sP"
      },
      "source": [
        "Notez que nous avons transcrit le tweet en rajoutant \\<sot\\> au début de la liste et \\<eot\\> à la fin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wpf_s6rRt4H6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 43339/43339 [00:00<00:00, 75746.61it/s]\n"
          ]
        }
      ],
      "source": [
        "# Parcours de la liste de tweets et création d'une liste de listes d'indices des caractères composant chaque tweet\n",
        "tweets_ind = []\n",
        "for tweet in tqdm(tweets):\n",
        "  tweet_ind = [char_to_ind['<sot>']]\n",
        "  for char in tweet:\n",
        "    tweet_ind.append(char_to_ind[char])\n",
        "  tweet_ind.append(char_to_ind['<eot>'])\n",
        "  tweets_ind.append(tweet_ind)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQs48m2B_B4P"
      },
      "source": [
        "Il est intéressant de visualiser l'ordre de grandeur des tweets une fois tokenizés :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p0TbX3S2rZFv"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Affichage d'un histogramme des longueurs des tweets tokenizés\n",
        "tweet_lengths = [len(tweet) for tweet in tweets_ind]\n",
        "plt.hist(tweet_lengths, bins=100)\n",
        "plt.title('Histogramme des longueurs des tweets')\n",
        "plt.xlabel('Longueur')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5WJiTlM_Kz1"
      },
      "source": [
        "La grande majorité des tweets ont moins de 150 caractères. Ceci permettra de choisir la taille du contexte un peu plus tard.\n",
        "\n",
        "\n",
        "Nous allons maintenant formater les données pour l'apprentissage. Notre modèle de langage sera un modèle *Many-to-many* qui prédira pour chaque token de la séquence le token suivant. Souvenez-vous, nous avons fait la même chose l'an passé en TP sur les RNN (à l'époque, avec des noms de famille).\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1TfgprY0yB4blIlRbHHr3S8UQB3nid5zo&sz=w1000\" width=600> </center>\n",
        "<caption><center> Génération de nom à l'aide d'une cellule récurrente</center></caption>\n",
        "\n",
        "Nous devons donc créer des matrices X et Y contenant des séquences de tweets, où les séquences de Y sont décalées d'un cran vers la droite.\n",
        "\n",
        "Pour cela, nous allons commencer par concaténer l'intégralité des tweets tokenizés du dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GevsK_karuVe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5785703\n"
          ]
        }
      ],
      "source": [
        "# Concaténation de tous les tweets tokenizés dans une seule liste\n",
        "all_tweets_ind = []\n",
        "for tweet in tweets_ind:\n",
        "  all_tweets_ind += tweet\n",
        "\n",
        "print(len(all_tweets_ind))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1psZElFPAKUv"
      },
      "source": [
        "Nous allons maintenant fixer une taille de contexte. Ceci est un hyperparamètre que vous pourrez ajuster un peu plus tard, mais **gardez à l'esprit que les couches d'attention ont une complexité qui croit avec le carré de la taille du contexte**. Autrement dit, plus le contexte est petit, plus votre modèle sera rapide. Mais, bien sûr, moins il conservera de mémoire pour générer la fin d'un tweet à partir du début."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ywi7pAc4AjdN"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKvCNhQBAiyU"
      },
      "source": [
        "Pour créer les variables X et Y, nous allons découper l'intégralité de notre dataset en morceaux de longueur ```MAX_LEN```+1. Chaque morceau sera ensuite réparti entre la variable X (les ```MAX_LEN``` premiers tokens) et la variable Y (les ```MAX_LEN``` derniers tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ImHrV4gmr2wT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0, 36, 70, 2, 84, 86, 83, 70, 2, 85, 80, 2, 85, 86, 79, 70, 2, 74, 79, 2, 66, 79, 69, 2, 88, 66, 85, 68, 73, 2, 38, 80, 79, 66, 77, 69, 2, 54, 83, 86, 78, 81, 2, 80, 79, 2, 46, 66, 85, 70], [48, 74, 72, 73, 85, 2, 88, 74, 85, 73, 2, 38, 66, 87, 74, 69, 2, 46, 70, 85, 85, 70, 83, 78, 66, 79, 2, 66, 84, 2, 73, 70, 2, 81, 83, 70, 84, 70, 79, 85, 84, 2, 85, 73, 70, 2, 54, 80, 81, 2], [70, 79, 2, 46, 74, 84, 85, 2, 85, 80, 79, 74, 72, 73, 85, 3, 1, 0, 38, 80, 79, 66, 77, 69, 2, 54, 83, 86, 78, 81, 2, 88, 74, 77, 77, 2, 67, 70, 2, 66, 81, 81, 70, 66, 83, 74, 79, 72, 2, 80], [2, 54, 73, 70, 2, 56, 74, 70, 88, 2, 85, 80, 78, 80, 83, 83, 80, 88, 2, 78, 80, 83, 79, 74, 79, 72, 2, 85, 80, 2, 69, 74, 84, 68, 86, 84, 84, 2, 37, 70, 77, 70, 67, 83, 74, 85, 90, 2, 35, 81], [83, 70, 79, 85, 74, 68, 70, 2, 66, 79, 69, 2, 73, 74, 84, 2, 79, 70, 88, 2, 67, 80, 80, 76, 2, 54, 73, 74, 79, 76, 2, 46, 74, 76, 70, 2, 35, 2, 37, 73, 66, 78, 81, 74, 80, 79, 3, 1, 0, 38]]\n",
            "[[36, 70, 2, 84, 86, 83, 70, 2, 85, 80, 2, 85, 86, 79, 70, 2, 74, 79, 2, 66, 79, 69, 2, 88, 66, 85, 68, 73, 2, 38, 80, 79, 66, 77, 69, 2, 54, 83, 86, 78, 81, 2, 80, 79, 2, 46, 66, 85, 70, 2], [74, 72, 73, 85, 2, 88, 74, 85, 73, 2, 38, 66, 87, 74, 69, 2, 46, 70, 85, 85, 70, 83, 78, 66, 79, 2, 66, 84, 2, 73, 70, 2, 81, 83, 70, 84, 70, 79, 85, 84, 2, 85, 73, 70, 2, 54, 80, 81, 2, 54], [79, 2, 46, 74, 84, 85, 2, 85, 80, 79, 74, 72, 73, 85, 3, 1, 0, 38, 80, 79, 66, 77, 69, 2, 54, 83, 86, 78, 81, 2, 88, 74, 77, 77, 2, 67, 70, 2, 66, 81, 81, 70, 66, 83, 74, 79, 72, 2, 80, 79], [54, 73, 70, 2, 56, 74, 70, 88, 2, 85, 80, 78, 80, 83, 83, 80, 88, 2, 78, 80, 83, 79, 74, 79, 72, 2, 85, 80, 2, 69, 74, 84, 68, 86, 84, 84, 2, 37, 70, 77, 70, 67, 83, 74, 85, 90, 2, 35, 81, 81], [70, 79, 85, 74, 68, 70, 2, 66, 79, 69, 2, 73, 74, 84, 2, 79, 70, 88, 2, 67, 80, 80, 76, 2, 54, 73, 74, 79, 76, 2, 46, 74, 76, 70, 2, 35, 2, 37, 73, 66, 78, 81, 74, 80, 79, 3, 1, 0, 38, 80]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Création des variables X et Y\n",
        "# On découpe l'ensemble des tweets concaténés en MAXLEN+1 caractères consécutifs : les MAXLEN premiers sont les x et les MAXLEN derniers sont les y\n",
        "### A COMPLETER\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(round(len(all_tweets_ind) / (MAX_LEN+1))):\n",
        "    seq = all_tweets_ind[i*(MAX_LEN+1):(i+1)*(MAX_LEN+1)]\n",
        "    X.append(seq[0:MAX_LEN])\n",
        "    Y.append(seq[1:MAX_LEN+1])\n",
        "\n",
        "print(X[0:5])\n",
        "print(Y[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrB3NWbMBD5q"
      },
      "source": [
        "**Résultat attendu** :\n",
        "```\n",
        "[[ 0. 36. 70. ... 66. 85. 70.]\n",
        " [48. 74. 72. ... 80. 81.  2.]\n",
        " [70. 79.  2. ... 72.  2. 80.]\n",
        " ...\n",
        " [70.  2. 85. ... 90.  2. 85.]\n",
        " [79. 74. 72. ... 84. 81. 67.]\n",
        " [26. 58.  1. ... 17. 56. 46.]] [[36. 70.  2. ... 85. 70.  2.]\n",
        " [74. 72. 73. ... 81.  2. 54.]\n",
        " [79.  2. 46. ...  2. 80. 79.]\n",
        " ...\n",
        " [ 2. 85. 80. ...  2. 85. 80.]\n",
        " [74. 72. 73. ... 81. 67. 55.]\n",
        " [58.  1.  0. ... 56. 46. 77.]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjMWeLgjBMNj"
      },
      "source": [
        "**NB** : peut-être êtes vous choqué·e par le fait que certaines données de notre ensemble d'apprentissage se retrouvent ainsi \"à cheval\" sur deux tweets consécutifs, qui n'ont potentiellement rien à voir.\n",
        "\n",
        "Cela nous permet d'éviter l'écueil de devoir gérer les différences de longueur des tweets. Si nous avions choisi une taille de contexte de, disons, 300 (autour de la longueur maximum des tweets), nous aurions dû compléter la grande majorité des tweets avec un token spécial (pour faire du *padding*, \\<pad\\>). Ce token aurait alors été en écrasante majorité dans la base de données, et le modèle aurait eu plus de mal à apprendre à cause du déséquilibre de classes induit par cette solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E92yOKidtKjQ"
      },
      "source": [
        "## Création du modèle\n",
        "\n",
        "Pour cette étape, je vous renvoie aux explications du TP4 de la semaine passée car vous allez pouvoir réutiliser le code écrit à cette occasion.\n",
        "\n",
        "Du point de vue de l'implémentation, une unique chose sépare un réseau de type **GPT** (modèle décodeur, que nous implémentons cette semaine) d'un réseau de type **BERT** (modèle encodeur, implémenté au TP4) : la couche d'auto-attention.\n",
        "\n",
        "Dans **BERT**, qui est **B**idirectionnel, l'attention d'un token peut être portée à tous les tokens de la séquence, qu'ils soient situés avant ou après dans l'ordre de la séquence.\n",
        "\n",
        "Dans **GPT**, qui sera utilisé pour faire de la **G**énération, il est strictement interdit de porter attention à des tokens qui sont postérieurs au token courant ! En effet, comme l'objectif de **GPT** est de générer progressivement du texte, cela n'aurait aucun sens de porter attention à des tokens que nous allons générer dans le futur.\n",
        "\n",
        "A vous de lire attentivement la documentation de la classe [MultiHeadAttention](https://keras.io/api/layers/attention_layers/multi_head_attention/) pour déterminer quel paramètre l'on doit positionner poru masquer la couche d'attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "E8IhBm6QtMu3"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    # embed_dim désigne la dimension des embeddings maintenus à travers les différentes couches,\n",
        "    # et num_heads le nombre de têtes de la couche d'attention.\n",
        "    # DANS CETTE FONCTION, ON NE FAIT QUE DEFINIR LES COUCHES\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        # Définition des différentes couches qui composent le bloc\n",
        "        # Couche d'attention\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        # Première couche de Layer Normalization\n",
        "        self.layernorm1 = keras.layers.LayerNormalization()\n",
        "        # Couche Dense (Feed-Forward)\n",
        "        self.ffn = Dense(embed_dim, activation='relu')\n",
        "        # Deuxième couche de normalisation\n",
        "        self.layernorm2 = keras.layers.LayerNormalization()\n",
        "\n",
        "    # DANS CETTE FONCTION, ON APPELLE EXPLICITEMENT LES COUCHES DEFINIES DANS __init__\n",
        "    # ON PROPAGE DONC LES ENTREES inputs A TRAVERS LES DIFFERENTES COUCHES POUR OBTENIR\n",
        "    # LA SORTIE\n",
        "    def call(self, inputs):\n",
        "        # Application des couches successives aux entrées\n",
        "        out1 = self.att(inputs, inputs, use_causal_mask=True)\n",
        "        out2 = self.layernorm1(out1 + inputs)\n",
        "        out3 = self.ffn(out2)\n",
        "        out4 = self.layernorm2(out3 + out2)\n",
        "        return out4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5fmllcPDXv_"
      },
      "source": [
        "La partie Embedding est inchangée :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "F_4I_WsDtgmY"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Définition des différentes couches qui composent le bloc Embedding\n",
        "        # Embedding de mot\n",
        "        self.token_emb = keras.layers.Embedding(vocab_size, embed_dim)\n",
        "        # Embedding de position\n",
        "        self.pos_emb = keras.layers.Embedding(maxlen, embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Calcul de l'embedding à partir de l'entrée x\n",
        "        # ATTENTION : UTILISER UNIQUEMENT DES FONCTIONS TF POUR CETTE PARTIE\n",
        "        # Récupération de la longueur de la séquence\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # Création d'un vecteur [0, 1, ..., maxlen] des positions associées aux\n",
        "        # mots de la séquence (fonction tf.range)\n",
        "        positions = tf.range(maxlen)\n",
        "        # Calcul des embeddings de position\n",
        "        positions_emb = self.pos_emb(x)\n",
        "        # Calcul des embeddings de mot\n",
        "        words_emb = self.token_emb(x)\n",
        "        return positions_emb + words_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRw39S4IDbj8"
      },
      "source": [
        "Enfin, vous devez assembler un réseau qui ressemble à GPT (ci-dessous) :    \n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/thumbnail?id=1w1CyLROPq-EWMd-Spr6wR596QEx1KpNa&sz=w1000\" width=200> </center>\n",
        "<caption><center>  Schéma de l'architecture de GPT</center></caption>\n",
        "\n",
        "Je vous propose des valeurs pour EMBED_DIM et NUM_HEADS, mais vous êtes bien sûr libres de les modifier.\n",
        "\n",
        "Pour obtenir des résultats similaires aux miens, vous devez utiliser 3 blocs transformers, et terminer le réseaux avec une couche dense de 100 neurones, et enfin la couche de sortie (à vous de déterminer quoi y mettre !)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3oxBesfJtmFg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding_12 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_26            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">544,512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_27            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">544,512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_28            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">544,512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,900</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,049</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding_12 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m25,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_26            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m544,512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_27            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m544,512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_28            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m544,512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m12,900\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m149\u001b[0m)        │        \u001b[38;5;34m15,049\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,957</span> (6.44 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,686,957\u001b[0m (6.44 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,957</span> (6.44 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,957\u001b[0m (6.44 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "EMBED_DIM = 128  # Dimension de l'embedding pour chaque mot\n",
        "NUM_HEADS = 8  # Nombre de têtes d'attention\n",
        "VOCAB_SIZE = len(char_to_ind)\n",
        "### A COMPLETER\n",
        "inputs = keras.Input(shape=(MAX_LEN,))\n",
        "embedding_layer = TokenAndPositionEmbedding(vocab_size=VOCAB_SIZE, maxlen=MAX_LEN, embed_dim=EMBED_DIM)(inputs)\n",
        "transformer_layer_1 = TransformerBlock(embed_dim=EMBED_DIM, num_heads=NUM_HEADS)(embedding_layer)\n",
        "transformer_layer_2 = TransformerBlock(embed_dim=EMBED_DIM, num_heads=NUM_HEADS)(transformer_layer_1)\n",
        "transformer_layer_3 = TransformerBlock(embed_dim=EMBED_DIM, num_heads=NUM_HEADS)(transformer_layer_2)\n",
        "# globalAvgPooling = keras.layers.GlobalAveragePooling1D()(transformer_layer_3)\n",
        "linear = Dense(100, activation='relu')(transformer_layer_3)\n",
        "outputs = Dense(VOCAB_SIZE, activation='softmax')(linear)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q2hi_SmEbrI"
      },
      "source": [
        "Il reste à entraîner le modèle. Pour faire en sorte que ça ne soit pas trop long, je n'ai mis que 5 epochs, mais des entrainements plus longs conduisent à de meilleurs résultats.\n",
        "\n",
        "Grosse difficulté de cette partie : quelle fonction de coût devez-vous utiliser ? Un petit indice, vous la trouverez sur [cette page](https://keras.io/api/losses/) au rayon *Probabilistic losses*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "EEDWymGNuGOv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1743493317.379518   12852 service.cc:148] XLA service 0x7a56740026f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1743493317.379537   12852 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
            "2025-04-01 09:41:57.543807: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "W0000 00:00:1743493317.829677   12852 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
            "I0000 00:00:1743493318.199157   12852 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "2025-04-01 09:41:59.420077: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 160 bytes spill stores, 160 bytes spill loads\n",
            "\n",
            "2025-04-01 09:41:59.461597: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 208 bytes spill stores, 204 bytes spill loads\n",
            "\n",
            "2025-04-01 09:41:59.563342: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 64 bytes spill stores, 72 bytes spill loads\n",
            "\n",
            "2025-04-01 09:41:59.599087: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "2025-04-01 09:41:59.871501: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 32 bytes spill stores, 32 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:00.196471: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 160 bytes spill stores, 160 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:01.080904: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 208 bytes spill stores, 204 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:01.332148: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 64 bytes spill stores, 72 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:01.603245: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:01.691111: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 540 bytes spill stores, 408 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:01.782185: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 1192 bytes spill stores, 872 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:02.185093: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 932 bytes spill stores, 952 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:02.225918: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 164 bytes spill stores, 232 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:02.456903: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23_0', 344 bytes spill stores, 272 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:02.540043: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 48 bytes spill stores, 48 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:03.047673: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:03.100249: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:03.425975: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 336 bytes spill stores, 336 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:03.571473: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 1724 bytes spill stores, 1204 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:04.591948: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 44 bytes spill stores, 44 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:05.115810: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29_0', 152 bytes spill stores, 144 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:05.242404: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 76 bytes spill stores, 76 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:05.541331: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:06.416787: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:07.385112: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3383', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:07.571385: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3383', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:07.795019: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 40 bytes spill stores, 40 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:07.859937: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 76 bytes spill stores, 76 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:08.409698: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 40 bytes spill stores, 40 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:08.410844: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 96 bytes spill stores, 96 bytes spill loads\n",
            "\n",
            "2025-04-01 09:42:08.875884: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_55', 44 bytes spill stores, 44 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  13/7091\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 9ms/step - accuracy: 0.1093 - loss: 4.5927  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-01 09:42:14.275444: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'copy_fusion', 200 bytes spill stores, 200 bytes spill loads\n",
            "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 44 bytes spill stores, 44 bytes spill loads\n",
            "\n",
            "I0000 00:00:1743493334.304369   12852 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7086/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3799 - loss: 2.2819"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1743493392.743367   12853 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
            "2025-04-01 09:43:14.142679: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83_0', 32 bytes spill stores, 32 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:14.167561: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 156 bytes spill stores, 156 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:14.602746: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79_0', 32 bytes spill stores, 32 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:15.040322: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 156 bytes spill stores, 156 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:15.204062: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 32 bytes spill stores, 32 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:15.373489: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:15.412406: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 540 bytes spill stores, 408 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:15.858934: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_79', 32 bytes spill stores, 32 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:16.510558: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 936 bytes spill stores, 956 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:16.985021: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_83', 24 bytes spill stores, 24 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:17.723402: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 48 bytes spill stores, 48 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:18.441689: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_22', 104 bytes spill stores, 104 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:19.351062: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:20.071126: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23_0', 344 bytes spill stores, 272 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:20.235207: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3383', 16 bytes spill stores, 16 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:20.628504: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 164 bytes spill stores, 232 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:20.800126: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 1724 bytes spill stores, 1204 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:21.524611: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_23', 1192 bytes spill stores, 872 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:21.653449: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3383', 4 bytes spill stores, 4 bytes spill loads\n",
            "\n",
            "2025-04-01 09:43:22.618492: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_29_0', 152 bytes spill stores, 144 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7091/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 10ms/step - accuracy: 0.3800 - loss: 2.2817\n",
            "Epoch 2/5\n",
            "\u001b[1m   8/7091\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 8ms/step - accuracy: 0.4820 - loss: 1.8822  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-01 09:43:27.337506: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_6', 28 bytes spill stores, 28 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7091/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 8ms/step - accuracy: 0.5000 - loss: 1.7984\n",
            "Epoch 3/5\n",
            "\u001b[1m7091/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 8ms/step - accuracy: 0.5221 - loss: 1.7137\n",
            "Epoch 4/5\n",
            "\u001b[1m7091/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 8ms/step - accuracy: 0.5344 - loss: 1.6663\n",
            "Epoch 5/5\n",
            "\u001b[1m7091/7091\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 8ms/step - accuracy: 0.5429 - loss: 1.6349\n"
          ]
        }
      ],
      "source": [
        "### A COMPLETER\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "with tf.device('/GPU:0'):\n",
        "    history = model.fit(\n",
        "        np.array(X), np.array(Y), batch_size=16, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqjlnZgmu9xA"
      },
      "source": [
        "## Génération d'un résultat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ksStATE-Sz"
      },
      "source": [
        "Profitez du temps d'entrainement de la cellule précédente pour prendre un café et lire le code permettant d'implémenter la génération. Il n'y a rien à compléter mais ce code n'est pas si facile à comprendre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fvFrhasCvAd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obama is as to hell cystem go stom why the made forward claine\" Lighoam's spending ither the record of they didned to plies.\" - Thanks!\n"
          ]
        }
      ],
      "source": [
        "# Début de séquence : mettre '' si l'on veut générer un nom de zéro\n",
        "seed_seq = 'Obama is'\n",
        "\n",
        "i = 0\n",
        "\n",
        "# Création de la séquence qui va être fournie en entrée du réseau :\n",
        "# On ajoute un token <sos> au démarrage, et on transcrit en la séquence d'id correspondante\n",
        "input_seq = [char_to_ind['<sot>']]\n",
        "for s in seed_seq:\n",
        "  input_seq.append(char_to_ind[s])\n",
        "  i+=1\n",
        "\n",
        "last_char = -1\n",
        "\n",
        "\n",
        "# On génère des séquences de taille inférieure à MAX_LEN, et on s'arrête lorsque\n",
        "# l'on génère un token <eot> (id 1)\n",
        "while last_char != 1 and i < 200:\n",
        "  # La séquence d'entrée doit être de dimension BATCH_SIZE x MAX_LEN\n",
        "  # soit en fait ici 1 x MAX_LEN\n",
        "  input = np.zeros((MAX_LEN))\n",
        "  if i < MAX_LEN:\n",
        "    input[:i+1] = np.array(input_seq)\n",
        "  else:\n",
        "    input = np.array(input_seq[-MAX_LEN:])\n",
        "\n",
        "  input = np.expand_dims(input, 0)\n",
        "\n",
        "  # Prédiction du modèle sur la séquence en cours\n",
        "  pred = model.predict(input, verbose=0)\n",
        "  # En sortie, on a 1 x MAX_LEN x 149\n",
        "\n",
        "  # Échantillonnage du caractère généré à partir de la distribution de probabilité\n",
        "  # prédite par le modèle pour le dernier élément de la séquence\n",
        "  last_char = np.random.choice(VOCAB_SIZE, 1, p=pred[0, min(i, MAX_LEN-1)])[0]\n",
        "\n",
        "  # Ajout du caractère à la séquence générée\n",
        "  input_seq.append(last_char)\n",
        "  i += 1\n",
        "\n",
        "# Affichage du nom généré\n",
        "generated_tweet = ''\n",
        "for s in input_seq:\n",
        "  if s != 0 and s != 1:\n",
        "    generated_tweet+=ind_to_char[s]\n",
        "\n",
        "print(generated_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQvY_KbrFQ4s"
      },
      "source": [
        "Voici quelques exemples de résultats que j'obtiens après 5 epochs (la loss est autour de 1,5, les résultats s'améliorent un peu à mesure qu'elle baisse) :     \n",
        "\n",
        "```Obama is no manufacture, there's pruingistricty. Think IME DENDY DUNT NATED more to construction, the holden for working on foom reproud & thank you!pic.twitter.com/wNXxY03ETLbE```\n",
        "\n",
        "```Obama is a lot of @ CNN and the This is country.```\n",
        "\n",
        "```Obama is an oposet bad all times and hill out the world in me.We dected were progress on @ dmiasonstroning Bernie, VOTES and ar the crime night Donumer more,. Laput are . I create pla att to bluin Sco```\n",
        "\n",
        "Les résultats peuvent paraître décevants, mais la plupart des mots générés existent : c'est déjà satisfaisant, car le modèle les génère caractère par caractère !\n",
        "\n",
        "En augmentant la profondeur du réseau (nombre de blocs transformers), la dimension des *Embeddings* et bien sûr le nombre d'epochs, il est possible de faire mieux mais nous serons de toute façon limités par la Tokenization caractère qui rend le problème un peu plus difficile.\n",
        "\n",
        "Il est plus intéressant d'explorer plutôt le *fine-tuning* d'un modèle de langage déjà pré-entraîné.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuO5og2a8eU9"
      },
      "source": [
        "# Fine-tuning d'un petit LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmqOApuNGqX_"
      },
      "source": [
        "Commencez par installer toutes les librairies suivantes. Cela prendra un peu de temps (2e café), et vous risquez de devoir redémarrer votre machine virtuelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVzxY80C8lms"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3384gGKyG11u"
      },
      "source": [
        "Nous allons utiliser le modèle [OPT](https://arxiv.org/abs/2205.01068), pour *Open Pretrained Transformer* sorti par Meta en 2022. Il s'agit d'une collection de LLM de tailles allant de 125 millions de paramètres à 175 milliards de paramètres, pré-entraînés, et complètement libres d'utilisation.\n",
        "\n",
        "Comme nous l'avons vu la semaine passée, chargeons le Tokenizer et le modèle associés :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EWWAOEK85jH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycD0Jt_LHfNB"
      },
      "source": [
        "Dans le bloc suivant, nous préparons la base de données en créant un objet Dataset à partir de nos tweets (non tokenizés), et en les passant ensuite dans le Tokenizer de opt-125m."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf81Ptno-g1a"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": tweets})\n",
        "print(dataset)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDP5bCbEHzoB"
      },
      "source": [
        "Pour accélérer l'entraînement, nous utilisons une méthode que je n'ai pas eu le temps de vous présenter en cours :    [LoRA](https://arxiv.org/abs/2106.09685), pour *Low Rank Approximation*.\n",
        "\n",
        "L'idée de cette méthode est de modifier incrémentalement les poids synaptiques d'un réseau à l'aide d'une décomposition matricielle.\n",
        "\n",
        "Ainsi une matrice $W \\in \\mathbb{R}^{d \\times k}$ représentant les poids synaptiques d'une couche du réseau peut s'écrire\n",
        "$$ W' = W + AB$$\n",
        "où $A \\in \\mathbb{R}^{d \\times r}$ et $B \\in \\mathbb{R}^{r \\times k}$, avec $r$ un rang très petit par rapport aux dimensions $d$ et $k$.\n",
        "\n",
        "Les deux matrices $A$ et $B$ n'ont que peu de paramètres, et leur produit $AB$ sera de rang très faible. La méthode LoRA, utilisée pour le fine-tuning de LLM, consiste à bloquer les paramètres de $W$ et à autoriser uniquement l'entraînement de $A$ et $B$. Malgré leur faible nombre de paramètres, le résultat permet de modifier significativement les prédictions du LLM tout en étant plus léger à entraîner, et moins sujet au sur-apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihJ3xpgT-zcU"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtjdoNLxKBRR"
      },
      "source": [
        "Nous pouvons maintenant lancer le *fine-tuning*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb74wu8H-4hM"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCi-koaCKFKe"
      },
      "source": [
        "Le bloc suivant permet de définir le processus de génération de nouveaux tweets à partir de notre modèle. Il est possible d'initialiser ces tweets avec un prompt :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13iVN9ZuJC1d"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Configurer un pipeline de génération\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"Obama is \"\n",
        "results = generator(prompt, max_length=50, num_return_sequences=3, do_sample=True, temperature=0.7)\n",
        "\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"--- Génération {i+1} ---\")\n",
        "    print(result['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_RfIH9xKxWS"
      },
      "source": [
        "Si le code précédent est trop long à exécuter, vous pouvez charger les poids suivants, que j'ai moi-même fine-tunés pendant 3 epochs (~45 min sur Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u-HgjS3lKwiq"
      },
      "outputs": [],
      "source": [
        "!wget https://acarlier.fr/tp/fine_tuned_opt_125m.zip\n",
        "!unzip fine_tuned_opt_125m.zip\n",
        "\n",
        "# Path to the extracted model\n",
        "model_path = './'\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI-Vti6gNGad"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Configurer un pipeline de génération\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"Obama is \"\n",
        "results = generator(prompt, max_length=50, num_return_sequences=3, do_sample=True, temperature=0.7)\n",
        "\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"--- Génération {i+1} ---\")\n",
        "    print(result['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
